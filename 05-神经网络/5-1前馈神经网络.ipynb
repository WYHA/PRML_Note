{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前面讨论的回归线性模型和分类线性模型，它们基于固定非线性的基函数$\\phi_j(x)$的线性组合。形式如下：\n",
    "\n",
    "$$\n",
    "y(x, w)=f\\left(\\sum^M_{j=1}w_j\\phi_j(x)\\right)  \\tag{5.1}\n",
    "$$\n",
    "\n",
    "其中$f(\\cdot)$在**分类问题**中是一个线性激活函数，在**回归问题**中为恒等函数。这章的目的是推广这个模型。使得基函数$\\phi_j(x)$依赖于参数。\n",
    "\n",
    "首先，我们构造输入变量$x_1,...,x_D$的$M$个线性组合。形式如下：\n",
    "\n",
    "$$\n",
    "a_j = \\sum^D_{i=1}w^{(1)}_{ji}x_i+w^{(1)}_{j0}  \\tag{5.2}\n",
    "$$\n",
    "\n",
    "其中$j=1,...,M$。就像在线性回归中一样，上式中，我们有权重参数$w^{(1)}_{ji}$，有偏置参数$w^{(1)}_{j0}$。我们对上面的式子进行函数变换，如下：\n",
    "\n",
    "$$\n",
    "z_j=h(a_j) \\tag {5.3}\n",
    "$$\n",
    "\n",
    "其中$h(\\cdot)$是可微的非线性激活函数(activation function)，被变换的$a_j$叫做激活(activation) , 激活函数一般选择`S`形的函数，例如logistic sigmoid函数或者是双曲正切函数。根据公式（5.1），得到输出单元激活(output unit activation）\n",
    "\n",
    "$$\n",
    "a_k = \\sum^M_{j=1}w^{(2)}_{kj}z_j+w^{(2)}_{k0}  \\tag{5.4}\n",
    "$$\n",
    "\n",
    "其中$k=1,...,K$，且K是输出的总数量，这个变换对应神经网络的第二层。最后，使用一个恰当的激活函数对输出单元激活(5.4)进行变换。得到神经网络的一组输出$y_k$。对标准的**回归问题**，激活函数是恒等函数。从而$y_k=a_k$。类似的，对多个二元分类问题，每个输出单元可以使用logistic sigmoid函数变换。即：\n",
    "\n",
    "$$\n",
    "y_k=\\sigma(a_k)\n",
    "$$\n",
    "\n",
    "其中\n",
    "\n",
    "$$\n",
    "\\sigma(a)=\\frac{1}{1+exp(-a)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
