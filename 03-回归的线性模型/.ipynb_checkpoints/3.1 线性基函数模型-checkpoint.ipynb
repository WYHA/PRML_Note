{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.0 前言\n",
    "---\n",
    "在监督学习中，最大的两个方向就是回归（连续）和分类（离散）。在大二（？）之前一直不懂回归这个词语，知道学习了《应用回归分析》这门课程，才知道，恩哦。原来回归说的是这个。书中的举的是一个关于身高“向平均回归”的例子。个子高的父母倾向于生相对矮的孩子，个子矮的父母倾向于生个子相对高的孩子，从而使得后代的身高朝向某一个中间水平回归。具体的可以看看【http://blog.sina.com.cn/s/blog_75c57f600102wjxa.html】\n",
    "\n",
    "\n",
    "假设给一个数据集${x_n}$。其中有$N$个观测值。这$N$个观测对应了$N$个目标值${t_n}$。现在的问题是：**在给定新的一个（或多个）观测$x$，如何预测目标值$t$**\n",
    "\n",
    "- 1）我们可以使用一个函数$y(x)$来表示观测$x$和目标值$t$的关系。这就可以输入新的$x$就会得出目标值了。\n",
    "- 2) 可以建立一个概率分布$p(t|\\mathrm{\\bf{x}})$。表示了再知道了$x$的情况下，$t$的值是多少的概率。\n",
    "\n",
    "最简单的回归模型是线性回归：\n",
    "$$\n",
    "y(\\mathrm{\\bf{x}},\\mathrm{\\bf{w}})=w_{0}+w_{1}x_{1}+...+w_{D}x_{D} \\tag{3.1}\n",
    "$$\n",
    "\n",
    "在这个线性回归模型中，它既是参数$\\mathrm{\\bf{w}}$的线性函数，也是$\\mathrm{\\bf{x}}$的线性函数。现实中很多情况都不是使用一条简单的线模拟的。所以，我们将模型修改为下面形式：\n",
    "$$\n",
    "y(\\mathrm{\\bf{x}},\\mathrm{\\bf{w}}) = w_{0}+\\sum _{j=1}^{M-1}w_{j}\\phi_{j}(x) \\tag{3.2}\n",
    "$$\n",
    "\n",
    "**其中$\\phi_{j}(x)$叫做基函数**， $w_{0}$是一个偏置项。通过额外的虚基函数$\\phi_{0}(x)=1$，此时：\n",
    "$$\n",
    "y(\\mathrm{\\bf{x}},\\mathrm{\\bf{w}})=\\sum _{j=0}^{M-1}w_{j}\\phi_{j}(x)=w^{T}x\n",
    "$$\n",
    "\n",
    "公式(3.2)是参数$\\mathrm{\\bf{w}}$的线性函数。虽然这样简化了模型的分析，但是依然会有局限性。\n",
    "\n",
    "**在第1章中讨论的多项式拟合的模型中，使用的基函数是$\\phi_j(x)=x^j$。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.1.1 最大似然与最小平方\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**这一小节主要讲了最大似然和最小平方在回归模型中的应用和区别**\n",
    "\n",
    "### 最大似然\n",
    "最大似然就是利用已知的样本值，推导出最有可能的参数值。从概率的观点来看待回归问题：\n",
    "\n",
    "- 1）将高斯噪声加入到公式（3.2）中\n",
    "我们知道，当预测一个值时，知道的信息越多，预测越准确。比如预测某人的身高，我们知道父母的身高、该家庭的年收入、饮食、运动情况比只知道父母的身高预测的更加准确。但是很多情况下，我们是不知道其他信息的。这时加入一个高斯噪声来表示其他因素对目标值的影响。\n",
    "$$\n",
    "t = y(x,w) + \\epsilon\n",
    "$$\n",
    "\n",
    "其中$\\epsilon \\sim \\cal{N}(0, \\sigma^2)$，所以有：\n",
    "$$\n",
    "p(t|x,w,\\beta)=N(t|y(x,w),\\beta^{-1})\n",
    "$$\n",
    "\n",
    "**以下是推导过程**\n",
    "![](https://raw.githubusercontent.com/data2world/PRML_Note/master/IMG/CH03/else1.jpg)\n",
    "![](https://raw.githubusercontent.com/data2world/PRML_Note/master/IMG/CH03/else2.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面是手推最大似然和最小平方（最小二乘法）。\n",
    "\n",
    "- **(1)** 可以看到，当模型加入高斯噪声后，最大似然去掉常数项之后：\n",
    "$$\n",
    "\\mathrm{max}(L) = -\\frac{1}{2}\\beta \\sum_{n=1}^N\\{t_n-w^T\\phi(x_n)\\}^2\n",
    "$$\n",
    "就等价于最小平方\n",
    "\n",
    "$$\n",
    "\\mathrm{min}\\{\\frac{1}{2}\\beta \\sum_{n=1}^N\\{t_n-w^T\\phi(x_n)\\}^2\\}\n",
    "$$\n",
    "\n",
    "- **(2)** 规范方程(normal equation)\n",
    "\n",
    "$$\n",
    "w_{ML}=(X^TX)^{-1}X^Tt\n",
    "$$\n",
    "这个方程叫做最小平方的规范方程, 把表示为基函数的形式就是：\n",
    "\n",
    "$$\n",
    "w_{ML}=(\\Phi^T\\Phi)^{-1}\\Phi^Tt\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.1.2 最小平方的几何描述\n",
    "---\n",
    "**最小二乘法中的几何意义是高维空间中的一个向量在低维子空间的投影**\n",
    "\n",
    "\n",
    "![](https://raw.githubusercontent.com/data2world/PRML_Note/master/IMG/CH03/3.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上图中的$\\mathrm{\\bf{t}}=(t_1,...,t_N)^T$是我们的目标值。\n",
    "\n",
    "- 点乘$<\\mathrm{\\bf{v}}, \\mathrm{\\bf{w}}>$：\n",
    "$$\n",
    "\\mathrm{\\bf{v}} \\cdot \\mathrm{\\bf{w}} = \\mathrm{\\bf{v}}^T\\mathrm{\\bf{w}} = \\sum_{i=1}^n v_iw_i\n",
    "$$\n",
    "\n",
    "- 范数$||\\mathrm{\\bf{v}}||$:\n",
    "$$\n",
    "||\\mathrm{\\bf{v}}||=\\sqrt{<\\mathrm{\\bf{v}}, \\mathrm{\\bf{v}}>}=\\sqrt{\\sum_{i=1}^nv_i^2}\n",
    "$$\n",
    "\n",
    "那么：\n",
    "$$\n",
    "\\mathrm{cos}\\theta=\\frac{<\\mathrm{\\bf{v}}, \\mathrm{\\bf{w}}>}{||\\mathrm{\\bf{v}}|| ||\\mathrm{\\bf{w}}||}\n",
    "$$\n",
    "\n",
    "如果$\\mathrm{cos}\\theta=0$。那么，向量$\\mathrm{\\bf{v}}$和$\\mathrm{\\bf{w}}$正交。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## 3.1.3 顺序学习\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "同在第二章的时候，顺序估计方法。当涉及到求解过程涉及到一次处理整个数据集的时候，我们需要使用在线算法来提高效率。也就是说，每次考虑一个数据点，下次学习再更新数据点。顺序学习的优化算法可以使用随机梯度算法（SGD）。如果你知道梯度下降算法（对整个数据集），就会明白顺序学习使用SGD的原因了。\n",
    "我们知道梯度下降每次都会计算整个数据的梯度，而在这里。我们定义$E=\\sum_nE_n$。随机梯度像如下的方式做更新参数$w$：\n",
    "\n",
    "$$\n",
    "w^{(\\tau+1)}=w^{\\tau}-\\eta \\nabla E_n \\tag{3.22}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "其中$\\tau$是迭代次数。$\\eta$是学习率，此外我们需要初始化一个参数向量$w_0$。对平方和误差函数有:\n",
    "\n",
    "$$\n",
    "w^{(\\tau+1)}=w^{\\tau}+\\eta(t_n-w^{(\\tau)^T}\\phi_n)\\phi_n \\tag{3.23}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sto_gd(x, t, w, eta, n, num_of_iter):\n",
    "    \"\"\"\n",
    "    SGD简单实现\n",
    "    \"\"\"\n",
    "    cost = 0\n",
    "    x_t = x.transpose()\n",
    "    for i in range(num_of_iter):\n",
    "        dot_v = np.dot(w, x)\n",
    "        cost = np.sum((t-dot_v)**2)/(2*n)\n",
    "        gradient = np.dot(x_t, t-dot_v)/n\n",
    "        w = w - eta * gradient\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.1.4 正则化最小平方\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当模型遇到过拟合问题时（或不遇到也可以）添加正则化来“惩罚”模型的复杂度。此时，我们的误差函数是:\n",
    "\n",
    "$$\n",
    "E_D(w)+\\lambda E_W(w)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$\\lambda$是正则化系数（正则化的权重大小）。比如我们对前面的平方和误差函数添加一个正则项。那么\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{n=1}^N\\{t_n-w^T\\phi(x_n)\\}^2 + \\frac{\\lambda}{2} w^Tw  \\tag{3.27}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到误差函数是关于参数$w$的二次函数（L2正则化），即可以求出解析解。一般的：\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\sum_{n=1}^N\\{t_n-w^T\\phi(x_n)\\}^2 + \\frac{\\lambda}{2} \\sum_{j=1}^M|w_j|^q  \\tag{3.28}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- q=1，就是常说的L1正则化，Lasso\n",
    "- q=2，就是L2正则化。Ridge\n",
    "\n",
    "下图给出了不同$q$值下的正则化函数的轮廓线。\n",
    "![](https://raw.githubusercontent.com/data2world/PRML_Note/master/IMG/CH03/3.3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.1.5 多个输出\n",
    "---\n",
    "\n",
    "下面考虑预测多个输出的情况。我们将多个输出当成一个向量。记做目标向量$t$。常用的方法是对所有的目标向量使用同样的基函数进行建模。同上面的单变量一样，使用似然函数后，进行参数的估计。就可以求解了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reference\n",
    "---\n",
    "\n",
    "- [1] http://www.dbs.ifi.lmu.de/Lehre/MaschLernen/SS2016/Skript/BasisFunctions2016.pdf\n",
    "- [2] http://people.stat.sfu.ca/~lockhart/richard/350/08_2/lectures/Geometry/web.pdf\n",
    "- [3] http://www.stat.wisc.edu/~ifischer/Intro_Stat/Lecture_Notes/APPENDIX/A2._Geometric_Viewpoint/A2.3_-_Least_Squares_Approximation.pdf\n",
    "- [4] http://www1.uis.no/ansatt/odegaard/teach/empir_finance_2018/lectures/ols/ols_lecture_slides.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
