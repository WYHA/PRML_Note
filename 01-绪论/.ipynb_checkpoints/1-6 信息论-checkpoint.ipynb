{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 信息论\n",
    "---\n",
    ">这一节，作者简单介绍了信息论的一些概念。更详细的资料和讨论，请参考（Viterbi and Omura, 1979;Cover and Thomas, 1991;MaxKay, 2003）\n",
    "\n",
    "信息内容的度量以来于某个概率分布$p(x)$。为了表达我们接受到的信息，我们寻找一个函数$h(x)$,它是概率$p(x)$的单调函数。如果我们有两个不相关的事件$x$和$y$那么我们观察到两个事件同时发生接收到的**信息之和**为\n",
    "$$\n",
    "h(x,y)=h(x)+h(y)\n",
    "$$\n",
    "而两个事件的概率关系：\n",
    "$$\n",
    "p(x, y) = p(x)p(y)\n",
    "$$\n",
    "可以看出概率关系和信息量的多少有一定的对数关系，因此：\n",
    "$$\n",
    "h(x)-log_2p(x)\n",
    "$$\n",
    "其中负号保证了信息为正数或者是零。不难看出，概率越低（不确定性越大）信息量越多（大），$h(x)$的单位是比特（bit，binary digit）\n",
    "接着，我们用熵来评价整个随机变量x平均的信息量，而平均最好的量度就是随机变量的期望，即熵的定义如下：\n",
    "\n",
    "$$\n",
    "H[x]=-\\sum_{x}p(x)log_2p(x)\n",
    "$$\n",
    "\n",
    "header 1 | 1| 2| 3| 4| 5| 6| 7|8|\n",
    "---|--- |--- |--- |--- |--- |--- |---|---|\n",
    "随机变量 | a|b|c|d|e|f|g|h|\n",
    "状态|1/2|1/4|1/8|1/16|1/64|1/64|1/64|1/64| \n",
    "\n",
    "根据公式可以计算出熵为2bit\n",
    "![image](https://raw.githubusercontent.com/data2world/PRML_Note/master/IMG/CH01/1.30.png)\n",
    "\n",
    "作者以下将$\\log$换为$\\ln$为了后续章节思考起来更加方便。\n",
    "\n",
    "如上图所示，如果分布$p(x_i)$在几个值周围有尖锐的峰值，熵就会相对降低，如果相对平稳地跨过许多值，那么熵就会很高。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.6.1 相对熵和互信息\n",
    "\n",
    "我们已经知道了，**信息熵是衡量随机变量或者整个系统的不确定性，不确定性越大，熵越大，呈正相关关系**。\n",
    "\n",
    "每一个系统都会有一个真实的概率分布，我们根据真实的概率分布找到一个最优的策略，以最小的代价消除系统的不确定性，这个\"大小\"就是信息熵。而如果我们以非真实的分布来选择策略来消除系统的不确定性，这个\"大小\"就是交叉熵。\n",
    "$$\n",
    "\\sum_{k=1}^Np_k\\log_2\\frac{1}{q_k}\n",
    "$$\n",
    "\n",
    "其中$p_k$表示真实分布，而$q_k$表示非真实分布。**交叉熵越低，则策略越好**，所以在机器学习中，我们需要最小化交叉熵，这样我们的策略才会越接近最优策略。\n",
    "\n",
    "\n",
    "我们又如何去衡量不同策略之间的差异呢？**相对熵**，顾名思义，**相对熵是用来衡量两个取值为正的函数或概率分布之间的差异**\n",
    "\n",
    "$$\n",
    "KL(p||q)=-\\int p(x)\\ln q(x) \\mathrm{d}x - (-\\int p(x)\\ln p(x) \\mathrm{d}x) \\\\\\\\\n",
    "KL(p||q)= -\\int p(x)\\ln\\{\\frac{q(x)}{p(x)}\\}\\mathrm{d}x\n",
    "$$\n",
    "这被称为分布p(x)和分布q(x)之间的**相对熵（relative entropy）**，或者叫KL散度（Kullback and Leibler, 1951）。相对熵不是一个对称量，即$KL(p||q) \\neq  KL(q||p)$\n",
    "\n",
    "- 先介绍凸函数（convex function）的概念\n",
    "\n",
    "如果⼀个函数具有如下性质：每条弦都位于函数图像或其上⽅（如下图所⽰），那么我们说这个函数是凸函数。\n",
    "\n",
    "![](https://raw.githubusercontent.com/data2world/PRML_Note/master/IMG/CH01/1.31.png)\n",
    "\n",
    "如图所示，我们可以将位于$[a, b]$之间的任何一个$x$的值都可以写成$\\lambda a+(1-\\lambda)b$，其中$0\\leq \\lambda \\leq 1 $，弦上对应的点可以写成$\\lambda f(a)+(1-\\lambda)f(b)$。函数对应的值可以写为$f(\\lambda a + (1-\\lambda)b)$。所以凸函数具有以下的性质：\n",
    "\n",
    "$$\n",
    "f(\\lambda a + (1-\\lambda)b)\\leq\\lambda  a+(1-\\lambda)b   \n",
    "$$\n",
    "\n",
    "**典型的凸函数有:** $x^2$，$x\\ln x (x>0)$\n",
    "\n",
    "现在要证明，$KL$散度满足$KL(p||q)\\geq0$，并且当且仅当$p(x)=q(x)$时等号成立。\n",
    "\n",
    "\n",
    "使用归纳法，可以证明凸函数满足：\n",
    "\n",
    "$$\n",
    "f(\\sum^M_{i=1}\\lambda_i x_i)\\leq \\sum^M_{i=1}\\lambda_if(x_i)\n",
    "$$\n",
    "\n",
    "如果将$\\lambda_i$看成取值为${x_i}$的**离散变量**$x$的概率分布，那么上面的公式可以写成:\n",
    "\n",
    "$$\n",
    "f(E[x])\\leq E[f(x)]\n",
    "$$\n",
    "\n",
    "就是Jensen不等式，即函数的期望大于期望的函数。\n",
    "\n",
    "对**连续变量**，Jensen不等式的形式为\n",
    "$$\n",
    "f(\\int xp(x)\\mathrm {d}x) \\leq \\int f(x)p(x)\\mathrm {d}x\n",
    "$$\n",
    "\n",
    "那么对$KL$散度，我们有:\n",
    "\n",
    "$$\n",
    "KL(p||q)=-\\int p(x)\\ln{\\frac{q(x)}{p(x)}}\\mathrm{d}x \\geq -\\ln \\int q(x) \\mathrm{d}x=0\n",
    "$$\n",
    "\n",
    "因为$-\\ln x$是凸函数。又$\\int q(x) \\mathrm{d}x=1$\n",
    "\n",
    "假设数据通过未知分布$p(x)$生成，我们想要对$p(x)$建模。我们可以试着使用⼀些参数分布$q(x|\\theta)$来近似这个分布。确定$\\theta$的方式是最小化$p(x)$和$q(x|\\theta)$之间关于$\\theta$的$KL$散度。单事实上，我们并不知道$p(x)$。。。【尴尬】，不过我们想到：\n",
    "\n",
    "*如果我们给定有限数量的N个点，这些点满足某个概率分布或者概率密度函数，那么期望可以通过求和的方式估计。*\n",
    "\n",
    "$$\n",
    "E[f] \\simeq  \\frac{1}{N}\\sum f(x_n)\n",
    "$$\n",
    "\n",
    "**所以**，假设我们已经观察到服从分布$p(x)$的有限数量的训练点$x_n$，其中$n=1,...,N$，那么根据上述公式近似，即：\n",
    "\n",
    "$$\n",
    "KL(p||q) \\simeq \\frac{1}{N}\\sum^N_{n=1}\\{-\\ln q(x_n|\\theta)+\\ln p(x_n)\\}\n",
    "$$\n",
    "\n",
    "可以看出，该公式的第二项和$\\theta$无关，第一项是$\\theta$的负对数似然函数，我们对该公式最小化，就是最大化似然函数。\n",
    "\n",
    "**下面考虑两个变量组成的数据集**\n",
    "\n",
    "$p(x,y)$给出两个变量$x$和变量$y$组成的数据集。如果变量的集合是独立的，那么他们的联合分布可以分解为边缘分布的乘积$p(x,y)=p(x)p(y)$。**但是如果变量不独立**，那么我们可以通过考察联合概率分布与边缘概率分布乘积之间的$\\mathrm{KL}$散度来判断他们是否\"接近\"于相互独立。此时，$\\mathrm{KL}$散度为：\n",
    "\n",
    "$$\n",
    "I[x,y]=\\mathrm{KL}(p(x,y)||p(x)p(y))= -\\iint p(x,y)\\ln(\\frac{p(x)p(y)}{p(x,y)})\\mathrm{d}x\\mathrm{d}y\n",
    "$$\n",
    "\n",
    "这被称为变量$x$和变量$y$之间互信息（mutual information）。根据$\\mathrm{KL}$散度的性质，可以看到$I[x,y]\\geq 0$，当且仅当$x$和$y$相互独立时等号成立。使⽤概率的加和规则和乘积规\n",
    "则，我们看到互信息和条件熵之间的关系为：\n",
    "\n",
    "$$\n",
    "I[x,y]=H[x]-H[x|y]=H[y]-H[y|x]\n",
    "$$\n",
    "\n",
    "**因此我们可以把互信息看成由于知道y值⽽造成的x的不确定性的减⼩（反之亦然）**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
